{"cells":[{"cell_type":"markdown","metadata":{"id":"WwMhTIp3q_N0"},"source":["# Covid-19 Risk Factor Predictor\n","\n","\n","## Our Contribution\n","**We predict important risk factors for COVID-19 and we rank them by importance**. Risk factors can be characteristics, conditions, or behaviours. \n","\n","We build a **knowledge graph** by merging  entities and relations that we extract from the **CORD-19 corpus** with the **[Open Targets Genetics](https://genetics-docs.opentargets.org/)** dataset and the [Mondo Disease Ontology](https://www.ebi.ac.uk/ols/ontologies/mondo). This is done to build a graph that merges COVID-19 facts extracted from bleeding-edge literature and known facts on similar diseases stored in datasets online. We achieve such goal despite the absence of COVID-19 from OpenTargets.\n","\n","Loosely inspired by [Mendelian Randomization](https://en.wikipedia.org/wiki/Mendelian_randomization), we train a **[graph representation learning model](https://github.com/Accenture/AmpliGraph)** that learns vector representations of the concepts in the graph (i.e. **graph embeddings**). Such operation leverages the topology and the semantics of the graph to discover new associations between risk factors and COVID-19. To sanity-check the trained embeddings, we carve out known risk factors-disease associations for better-studied conditions such as SARS, MERS, and we assess the predictive power of the model using agreed-upon evaluation protocol and metrics. Our model reaches a validation mean reciprocal rank MRR=0.07.\n","\n","Finally, we use the knowledge graph embeddings to infer how likely COVID-19 is impacted by a risk factor and we return a **ranked list of predicted risk factors** for COVID-19.\n","Our results can be used either to test if a suspected risk factor applies to COVID-19, or to discover unsuspected risk factors.\n","\n","**The complete pipeline of the system is presented below**:\n","\n","![pipeline](https://kagglecovid.s3-eu-west-1.amazonaws.com/covid19_pipeline.jpg)\n","\n","\n","### Benefits\n","* First machine learning system that returns a ranked list of predited risk factors for COVID-19.\n","* The knowledge graph can easily be extended and updated with additional information (e.g. paper citation network, additional genomic datasets, etc.).\n","* Graph representation learning exploits relations between concepts shared across articles, and it is scalable to support larger graphs (up to 500+M edges).\n","\n","### Known Limitations\n","* In this Phase 1 prototype, we limited to CORD-19 **biorxiv articles** only.\n","* Our approach combining paper annotation with Open Targets assumes genetics and genomics play a role in the impact on COVID-19.\n","* The NER step likely missed some entities in the text and mis-identified others. Our second step mapping these entities identified as keyword into entities identified with unique identifier introduces a second level of possible mapping errors;\n","* The risk factor surfaced are correlations emerging from the structure of the graph, not causations. Our graph now an association network associating papers to the things they are about, and associating these things to each other;\n","* Due to time constraints, we did not carry out exhaustive model selection (hyperparameter tuning), to the detriment of predictive power.\n","* The embeddings have to be re-trained every time the network is changed (e.g. when new papers are added). This is a known limitation of knowledge graph embedding models.\n","\n","### Future Work Planned for Phase 2\n","* Re-build the graph on all CORD-19 papers\n","* Validation of the final output by domain experts.\n","* For each COVID-19 - Risk Factor prediction, we will also provide an explanation, in the form of a meaningful and succint subgraph that mostly affected such prediction.\n","* Extensive and thorough hyperparameter tuning to improve predictive power.\n","* The machine learning model will be adapted to support the addition of new concepts (e.g. newly-published papers), without the need to re-train the model.\n","* The work described in this notebook will be part of a [wider pipeline](https://www.kaggle.com/cgueret/covid-19-risk-factor-predictor-future-work) aimed at discovering additional COVID-19 insights from the CORD-19 corpus. \n"]},{"cell_type":"markdown","metadata":{"id":"a4qmpIuzq_N1"},"source":["\n","### Results\n","\n","**Scroll down at the end of the notebook to see the list of COVID-19 risk factors predicted by our system.**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"u3SDMMJeq_N1"},"source":["# Part 1/3: NER\n","\n","The first component of our system is the named-entity recognition & relation extraction.\n","It reads the CORD-19 corpus and extracts entities and relations that will be used later on to generate the knowledge graph.\n","\n","![](https://kagglecovid.s3-eu-west-1.amazonaws.com/covid19_pipeline_1.jpg)\n","\n","**The NER pipeline performs the following operations**:\n","1. Extract concepts from the papers using Spacy and the models from [SciSpacy](https://allenai.github.io/scispacy/).\n","2. Turn the found keywords into identifiers using a mapping extracted from [Open Targets]((https://genetics-docs.opentargets.org/)) and the [Mondo Disease Ontology](https://www.ebi.ac.uk/ols/ontologies/mondo).\n","\n","The NER pipeline is reported in the cells below:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T08:33:28.464651Z","iopub.status.busy":"2022-05-02T08:33:28.464328Z","iopub.status.idle":"2022-05-02T08:33:35.22519Z","shell.execute_reply":"2022-05-02T08:33:35.224457Z","shell.execute_reply.started":"2022-05-02T08:33:28.464618Z"},"trusted":true},"outputs":[],"source":["!conda env list"]},{"cell_type":"code","execution_count":7,"metadata":{"_cell_guid":"8d3b059d-deaa-40e4-942c-fa7697180669","_uuid":"dc8caabb-ee79-4daf-9f5b-54229c925ac9","execution":{"iopub.execute_input":"2022-05-02T09:43:08.692460Z","iopub.status.busy":"2022-05-02T09:43:08.692175Z","iopub.status.idle":"2022-05-02T09:43:57.944876Z","shell.execute_reply":"2022-05-02T09:43:57.943987Z","shell.execute_reply.started":"2022-05-02T09:43:08.692408Z"},"id":"3MI1QB_jq_N2","trusted":true},"outputs":[],"source":["import spacy\n","spacy.prefer_gpu()\n","import json\n","from tqdm.auto import tqdm\n","from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","import gzip\n","# !pip install opentargets\n","# from opentargets import OpenTargetsClient\n","# !pip install scispacy\n","# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bionlp13cg_md-0.2.4.tar.gz\n","# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_ner_bc5cdr_md-0.2.4.tar.gz\n"]},{"cell_type":"markdown","metadata":{"id":"Q7hiBTe8BiDm"},"source":["This function looks at all the papers in the CORD-19 dataset and extract entities. \n","\n","Note to speed up processing time, we only process articles from biorxiv:"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T10:21:48.241110Z","iopub.status.busy":"2022-05-02T10:21:48.240844Z","iopub.status.idle":"2022-05-02T10:21:48.255138Z","shell.execute_reply":"2022-05-02T10:21:48.254394Z","shell.execute_reply.started":"2022-05-02T10:21:48.241085Z"},"id":"Zutlz3VIq_OM","trusted":true},"outputs":[],"source":["def extract_paper_annotations():\n","    \"\"\"\n","    This function looks at all the papers in the CORD-19 dataset and extract entities\n","    \"\"\"\n","    # Define the list of papers we will process\n","    #papers = [Path(\"/kaggle/input/CORD-19-research-challenge/biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/59eab95c43fdea01481fdbf9bae45dfe28ffc693.json\")]\n","    papers = [p for p in Path('/kaggle/input/CORD-19-research-challenge').glob('biorxiv_medrxiv/biorxiv_medrxiv/pdf_json/*.json')]\n","    #papers += [p for p in Path('/kaggle/input/CORD-19-research-challenge').glob('comm_use_subset/comm_use_subset/pdf_json/*.json')]\n","    #papers += [p for p in Path('/kaggle/input/CORD-19-research-challenge').glob('noncomm_use_subset/noncomm_use_subset/pdf_json/*.json')]\n","    #papers += [p for p in Path('/kaggle/input/CORD-19-research-challenge').glob('custom_license/custom_license/pdf_json/*.json')]\n","    print (len(papers)) \n","\n","    # Load the NLP models\n","    # nlp_model_bionlp13cg = spacy.load('/opt/conda/lib/python3.6/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.2.4') # For cells, genes, ...\n","    # nlp_model_bc5cdr = spacy.load('/opt/conda/lib/python3.6/site-packages/en_ner_bc5cdr_md/en_ner_bc5cdr_md-0.2.4') # For diseases\n","    nlp_model_bionlp13cg = spacy.load('/home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.2.4') # For cells, genes, ...\n","    nlp_model_bc5cdr = spacy.load('/home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/en_ner_bc5cdr_md/en_ner_bc5cdr_md-0.2.4') # For diseases\n","\n","    # The output will be one hashmap associating each paper to its annotations\n","    output = {}\n","\n","    # Process all the papers\n","    for paper in tqdm(papers):\n","        try:\n","            # Load the document\n","            document = json.loads(paper.read_text())\n","\n","            # Get the ID\n","            paper_id = document['paper_id']\n","            \n","            # Initialise its entry\n","            output[paper_id] = {}\n","            output[paper_id]['topics'] = {} # The different topic annotations grouped per type\n","            \n","            # Group the text by sections (took more than 9h to process!)\n","            #section_texts = {}\n","            #section_texts['abstract'] = []\n","            #for b in document['abstract']:\n","            #    section_texts['abstract'].append(b['text'])\n","            #for b in document['body_text']:\n","            #    section_texts.setdefault(b['section'], [])\n","            #    section_texts[b['section']].append(b['text'])\n","\n","            # Retrieve all the text\n","            texts = []\n","            for b in document['abstract']:\n","                texts.append(b['text'])\n","            if 'body_text' in document:\n","                for b in document['body_text']:\n","                    texts.append(b['text'])\n","            \n","            # Process the different sections to extract entities\n","            #for section,texts in section_texts.items():\n","            text = '.'.join(texts)\n","            for nlp_model in [nlp_model_bionlp13cg, nlp_model_bc5cdr]:\n","                tokens = nlp_model(text)\n","                for entity in tokens.ents:\n","                    topic_type = entity.label_\n","                    topic_value = str(entity.text)\n","                    output[paper_id]['topics'].setdefault(topic_type, set())\n","                    output[paper_id]['topics'][topic_type].add(topic_value)\n","            \n","        except Exception as e:\n","            print ('Error with {}'.format(paper))\n","            print (e)\n","\n","    # Turn the sets into lists to save them as JSON\n","    for paper_id in output.keys():\n","        for topic_type in output[paper_id]['topics'].keys():\n","            output[paper_id]['topics'][topic_type] = list(output[paper_id]['topics'][topic_type])\n","\n","    return output"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-05-02T10:21:52.785773Z","iopub.status.busy":"2022-05-02T10:21:52.785482Z","iopub.status.idle":"2022-05-02T10:21:53.446584Z","shell.execute_reply":"2022-05-02T10:21:53.445290Z","shell.execute_reply.started":"2022-05-02T10:21:52.785746Z"},"id":"ySq-s3Cvq_Og","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n"]},{"ename":"OSError","evalue":"[E050] Can't find model '/opt/conda/lib/python3.6/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.2.4'. It doesn't seem to be a Python package or a valid path to a data directory.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[1;32m/home/laiduy98/Documents/UP/TER/cose-19/notebooks/covid-19-risk-factor-predictor.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/laiduy98/Documents/UP/TER/cose-19/notebooks/covid-19-risk-factor-predictor.ipynb#ch0000007?line=0'>1</a>\u001b[0m \u001b[39m# Step 1 => get the keywords out of the paper abstract and content\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/laiduy98/Documents/UP/TER/cose-19/notebooks/covid-19-risk-factor-predictor.ipynb#ch0000007?line=1'>2</a>\u001b[0m annotations \u001b[39m=\u001b[39m extract_paper_annotations()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/laiduy98/Documents/UP/TER/cose-19/notebooks/covid-19-risk-factor-predictor.ipynb#ch0000007?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39mlen\u001b[39m(annotations\u001b[39m.\u001b[39mkeys()))\n","\u001b[1;32m/home/laiduy98/Documents/UP/TER/cose-19/notebooks/covid-19-risk-factor-predictor.ipynb Cell 7'\u001b[0m in \u001b[0;36mextract_paper_annotations\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/laiduy98/Documents/UP/TER/cose-19/notebooks/covid-19-risk-factor-predictor.ipynb#ch0000006?line=10'>11</a>\u001b[0m \u001b[39mprint\u001b[39m (\u001b[39mlen\u001b[39m(papers)) \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/laiduy98/Documents/UP/TER/cose-19/notebooks/covid-19-risk-factor-predictor.ipynb#ch0000006?line=12'>13</a>\u001b[0m \u001b[39m# Load the NLP models\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/laiduy98/Documents/UP/TER/cose-19/notebooks/covid-19-risk-factor-predictor.ipynb#ch0000006?line=13'>14</a>\u001b[0m nlp_model_bionlp13cg \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39m/opt/conda/lib/python3.6/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.2.4\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39m# For cells, genes, ...\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/laiduy98/Documents/UP/TER/cose-19/notebooks/covid-19-risk-factor-predictor.ipynb#ch0000006?line=14'>15</a>\u001b[0m nlp_model_bc5cdr \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39m/opt/conda/lib/python3.6/site-packages/en_ner_bc5cdr_md/en_ner_bc5cdr_md-0.2.4\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m# For diseases\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/laiduy98/Documents/UP/TER/cose-19/notebooks/covid-19-risk-factor-predictor.ipynb#ch0000006?line=16'>17</a>\u001b[0m \u001b[39m# The output will be one hashmap associating each paper to its annotations\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/__init__.py?line=29'>30</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/__init__.py?line=30'>31</a>\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/__init__.py?line=31'>32</a>\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/__init__.py?line=35'>36</a>\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/__init__.py?line=36'>37</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/__init__.py?line=37'>38</a>\u001b[0m     \u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/__init__.py?line=38'>39</a>\u001b[0m \n\u001b[1;32m     <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/__init__.py?line=39'>40</a>\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/__init__.py?line=48'>49</a>\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/__init__.py?line=49'>50</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/__init__.py?line=50'>51</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/__init__.py?line=51'>52</a>\u001b[0m         name, vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig\n\u001b[1;32m     <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/__init__.py?line=52'>53</a>\u001b[0m     )\n","File \u001b[0;32m~/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/util.py:427\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/util.py?line=424'>425</a>\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/util.py?line=425'>426</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/laiduy98/miniconda3/envs/ds/lib/python3.9/site-packages/spacy/util.py?line=426'>427</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n","\u001b[0;31mOSError\u001b[0m: [E050] Can't find model '/opt/conda/lib/python3.6/site-packages/en_ner_bionlp13cg_md/en_ner_bionlp13cg_md-0.2.4'. It doesn't seem to be a Python package or a valid path to a data directory."]}],"source":["# Step 1 => get the keywords out of the paper abstract and content\n","annotations = extract_paper_annotations()\n","print (len(annotations.keys()))"]},{"cell_type":"markdown","metadata":{"id":"na_i-q-_q_OH"},"source":["# Part 2/3: Knowledge Graph Generation\n","\n","In this section we generate the knowledge graph (KG) that part 3/3 will use to predict COVID-19 risk factors associations.\n","\n","![](https://kagglecovid.s3-eu-west-1.amazonaws.com/covid19_pipeline_2.jpg)\n","\n","### Knowledge Graph Schema\n","The knowledge graph models relations between CORD-19 papers (extracted by part 1/3) and Genes, Diseases retrieved from [Open Targets Genetics](https://genetics-docs.opentargets.org/).\n","This is the schema of the knowledge graph that we generate to predict covid19-specific risk factors:\n","\n","![](https://kagglecovid.s3-eu-west-1.amazonaws.com/covid_kg_ontology_v6_resized.png)\n","\n","+ **Paper**: an article from the CORDI-19 corpus.\n","+ **Target**: a gene that occurs in at least one CORD-19 paper, and that may be associated to a Disease.\n","+ **Disease**: following OpenTargets terminology, this class represents characteristics, conditions, and behaviours. The risk factors predicted by our system belowngs here.\n","\n","+ **isAboutGene**: connects a CORDI-19 paper to a target gene.\n","+ **isAboutDisease**: connects a CORDI-19 paper to a Disease (i.e. a characteristic, condition, or behaviour - following OpenTargets terminology).\n","+ **isASsociatedTo**: connects a target gene to a Disease (i.e. a characteristic, condition, or behaviour - following OpenTargets terminology).\n","+ **belongsToTherapeuticArea**: associates a Disease to its therapeutic area, as provided by Open Targets.\n","+ **isASpecific**: this predicate associates a Disease to an higher-up element in the Open Targets hierarchy (ontological path connecting higher classes of diseases to more specific instances).\n","+ **hasGeneticClue**: this predicate connects a Disease to a characteristic, condition, or behaviour if there are genetic evidences that such connection exists. **As indicated in the picture and the example below, we predict facts that include this predicate and COVID-19 as subject**.\n","\n","The figure below shows some facts included in the knowledge graph. The figure includes `COVID-19` and the relations that must be predicted in part 3/3.\n","\n","![covid_Abox.jpg](https://kagglecovid.s3-eu-west-1.amazonaws.com/covid_Abox.jpg)\n","\n","The graph has 327,834 facts (i.e. edges) and 21,025 distinct concepts (i.e. nodes).\n","The screenshot below had been generated with [Gephi](https://gephi.org/), and gives a sense of the complexity and size of the data structure:\n","\n","![graph_gephi.jpg](https://kagglecovid.s3-eu-west-1.amazonaws.com/graph_gephi.jpg)\n","\n","### Knowledge Graph Construction\n","The cells below include the code required to generate the knowledge graph.\n","The most important steps are:\n","1. We use the entities extracted by Step 1/3.\n","3. We query Open Targets to enrich the paper annotations with additional information, to connect the topics of the papers to each other (see above for the description of the schema):\n","  + Link between diseases and genes\n","  + Link between diseases based on their gene similarity\n","  + Ontological path connecting higher classes of diseases to more specific instances"]},{"cell_type":"markdown","metadata":{"id":"7W72DiF5q_OP"},"source":["This function is used to generate a graph from the paper annotations:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4KPbP4WRq_OP"},"outputs":[],"source":["def get_paper_annotations_graph(annotations):\n","    \"\"\"\n","    This function is used to generate a graph from the paper annotations\n","    \n","    We will turn all the NLP annotations into concept identifiers using a list of terms extracted form Open Targets and the ontology MONDO. \n","    This is done using a basic exact string matching and all the non matching strings are ignored.\n","\n","    We extract a mapping \"disease name\" => \"disease identifier\" from Open Targets as the primary source, falling back on Mondo to fill the gaps. \n","    In particular one of the missing value in Open Targets right now is Covid-19 ... ;-)\n","    \"\"\"\n","    \n","    # Prepare a map to deal with all the different types of entities type recognized by Spacy and that may be found in the annotations\n","    ontology_map = {\n","        'DISEASE': {},\n","        'CANCER': {},\n","        'GENE_OR_GENE_PRODUCT': {}\n","    }\n","\n","    # TODO: If we want to keep more of the annotations returned by Spacy we should align:\n","    # From https://allenai.github.io/scispacy/ en_ner_bionlp13cg_md\n","    #  CANCER, ORGAN, TISSUE, ORGANISM, CELL, AMINO_ACID, GENE_OR_GENE_PRODUCT, \n","    #  SIMPLE_CHEMICAL, ANATOMICAL_SYSTEM, IMMATERIAL_ANATOMICAL_ENTITY, \n","    #  MULTI-TISSUE_STRUCTURE, DEVELOPING_ANATOMICAL_STRUCTURE, \n","    #  ORGANISM_SUBDIVISION, CELLULAR_COMPONENT\n","    # From https://allenai.github.io/scispacy/ en_ner_bc5cdr_md\n","    #  DISEASE, CHEMICAL\n","\n","    ########################\n","    # Get mappings for DISEASE\n","    ########################\n","    \n","    # Load the file from Open Targets and fill the hashmap in\n","    disease_list = pd.read_csv('https://storage.googleapis.com/open-targets-data-releases/20.02/output/20.02_disease_list.csv.gz', compression='gzip')\n","    disease_list['disease_full_name'] = disease_list['disease_full_name'].str.lower()\n","    print('Number of keywords in open targets:', len(set(disease_list['disease_full_name'].values)))\n","    for index, row in disease_list.iterrows():\n","        full_name = row['disease_full_name'].lower()\n","        identifier = row['efo_id']\n","        ontology_map['DISEASE'][full_name] = identifier\n","\n","    # Open Targets does not have Covid-19 in its list of diseases. We had it manually\n","    # To get the labels we ran the following query on http://www.ontobee.org/sparql\n","    #  select distinct ?s ?o where {\n","    #    {<http://purl.obolibrary.org/obo/MONDO_0100096> <http://www.geneontology.org/formats/oboInOwl#hasExactSynonym> ?o} \n","    #    union\n","    #    {<http://purl.obolibrary.org/obo/MONDO_0100096> <http://www.w3.org/2000/01/rdf-schema#label> ?o}\n","    #  }\n","    ontology_map['DISEASE']['2019 novel coronavirus infection'.lower()] = 'MONDO_0100096'\n","    ontology_map['DISEASE']['2019-nCoV infection'.lower()] = 'MONDO_0100096'\n","    ontology_map['DISEASE']['severe acute respiratory syndrome coronavirus 2'.lower()] = 'MONDO_0100096'\n","    ontology_map['DISEASE']['SARS-CoV-2'.lower()] = 'MONDO_0100096'\n","    ontology_map['DISEASE']['SARS-coronavirus 2'.lower()] = 'MONDO_0100096'\n","    ontology_map['DISEASE']['coronavirus disease 2019'.lower()] = 'MONDO_0100096'\n","    ontology_map['DISEASE']['COVID-19'.lower()] = 'MONDO_0100096'\n","    \n","    # Debug output\n","    print ('Number of keywords in map for diseases: {}'.format(len(ontology_map['DISEASE'])))\n","    \n","    \n","    ########################\n","    # Get mappings for CANCER\n","    ########################\n","\n","    # We will simply treat the \"CANCER\" annotations from Spacy as \"DISEASE\"\n","    ontology_map['CANCER'] = ontology_map['DISEASE']\n","    \n","    \n","    ########################\n","    # Get mappings for GENE_OR_GENE_PRODUCT\n","    ########################\n","    \n","    # Load a target list from Open Targets. It will be used to map gene keywords\n","    target_list = pd.read_csv('https://storage.googleapis.com/open-targets-data-releases/20.02/output/20.02_target_list.csv.gz', compression='gzip')\n","    target_list['hgnc_approved_symbol'] = target_list['hgnc_approved_symbol'].str.lower()\n","    print('Number of genes in open targets:', target_list['hgnc_approved_symbol'].nunique())\n","    for index, row in target_list.iterrows():\n","        full_name = row['hgnc_approved_symbol']\n","        identifier = row['ensembl_id']\n","        ontology_map['GENE_OR_GENE_PRODUCT'][full_name] = identifier\n","    \n","\n","    ########################\n","    # Turn the paper annotations into a graph\n","    ########################\n","    graph = []\n","    predicates = {\n","        'DISEASE': 'isAboutDisease',\n","        'CANCER': 'isAboutDisease',\n","        'GENE_OR_GENE_PRODUCT': 'isAboutTarget'\n","    }\n","    # Go through all the papers\n","    for (paper_id, data) in annotations.items():\n","        # For each annotation topic try to find a match in the ontology map\n","        for (topic, values) in data['topics'].items():\n","            if topic in ontology_map:\n","                for value in values:\n","                    if value.lower() in ontology_map[topic]:\n","                        obj = ontology_map[topic][value.lower()]\n","                        graph.append([paper_id, predicates[topic], obj])\n","               \n","    return graph"]},{"cell_type":"markdown","metadata":{"id":"TE5p6St5q_OS"},"source":["This function will use the association data from Open Target to \n","    connect instances of Target and Disease in the graph.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MU5Hu_Eq_OS"},"outputs":[],"source":["def connect_targets_and_diseases(graph):\n","    \"\"\"\n","    This function will use the association data from Open Target to \n","    connect instances of Target and Disease in the graph.\n","    \n","    We at the same time connect diseases to therapeutic areas (instances of Disease)\n","    as this information is returned by the API\n","    \"\"\"\n","    \n","    # Get a list of all the targets (genes) and diseases currently in the graph\n","    targets = list(set([t[2] for t in graph if t[1] == 'isAboutTarget']))\n","    diseases = list(set([t[2] for t in graph if t[1] == 'isAboutDisease']))\n","    \n","    # Prepare a map of target => disease relations\n","    ot_output_associations = {}\n","    \n","    # Query OpenTargets for Target => Disease associations\n","    ot = OpenTargetsClient()\n","    for target in tqdm(targets):\n","        ot_output_associations.setdefault(target, set())\n","        search_results = ot.get_associations_for_target(target)\n","        if len(search_results) > 0 and search_results[0]['target']['id'] == target:\n","            for search_result in search_results:\n","                if search_result['association_score']['overall'] > 0.8:\n","                    disease = search_result['disease']['id']\n","                    ot_output_associations[target].add(disease)\n","                        \n","    # Query OpenTargets for Disease => Target associations\n","    for disease in tqdm(diseases): \n","        search_results = ot.get_associations_for_disease(disease)\n","        if len(search_results) > 0 and search_results[0]['disease']['id'] == disease:\n","            for search_result in search_results:\n","                if search_result['association_score']['overall'] > 0.8:\n","                    target = search_result['target']['id']\n","                    ot_output_associations.setdefault(target, set())\n","                    ot_output_associations[target].add(disease)\n","\n","    # Turn the output into new edges in the graph\n","    for target, diseases in ot_output_associations.items():\n","        for disease in diseases:\n","            # Target -> Disease relation\n","            graph.append([target, 'isAssociatedTo', disease])            "]},{"cell_type":"markdown","metadata":{"id":"TdbrHxLeq_OV"},"source":["This function leverages the disease similarity information computed by Open Targets to connect Diseases to each other. \n","Those links will later be used to find risk factors:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_nh4G5_q_OV"},"outputs":[],"source":["def connect_diseases_to_diseases(graph):\n","    \"\"\"\n","    This function leverages the disease similarity information computed by Open Targets\n","    to connect Diseases to each other. Those links will later be used to find risk factors.\n","    \"\"\"\n","    \n","    # Get a list of all the diseases currently in the graph.\n","    # We do that by looking at the objects of triples we know link to Diseases\n","    diseases = set([t[2] for t in graph if t[1] == 'isAboutDisease']) \n","    diseases = diseases | set([t[2] for t in graph if t[1] == 'isAssociatedTo']) \n","    \n","    # Query OpenTargets\n","    ot_output_diseases = {}\n","    ot = OpenTargetsClient()\n","    for disease in tqdm(diseases):\n","        ot_output_diseases[disease] = set()\n","        search_results = ot.get_similar_disease(disease)\n","        for search_result in search_results:\n","            if search_result['subject']['id'] == disease: # Safe guard\n","                ot_output_diseases[disease].add(search_result['object']['id'])\n","                \n","    # Turn the output we received into edges\n","    for src_disease, target_diseases in ot_output_diseases.items():\n","        for target_disease in target_diseases:\n","            graph.append([src_disease, 'hasGeneticClue', target_disease])"]},{"cell_type":"markdown","metadata":{"id":"2ReMxsR7q_OY"},"source":["This function adds to the graph the disease classification tree:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v-6601jaq_OY"},"outputs":[],"source":["def add_disease_classification(graph):\n","    \"\"\"\n","    This function adds to the graph the disease classification tree.\n","    See, for example, https://www.targetvalidation.org/disease/EFO_0005774 .\n","    \"\"\"\n","\n","    # Get a list of all the diseases in the graph\n","    diseases = set([t[2] for t in graph if t[1] == 'isAboutDisease']) \n","    diseases = diseases | set([t[2] for t in graph if t[1] == 'isAssociatedTo']) \n","    diseases = diseases | set([t[2] for t in graph if t[1] == 'hasGeneticClue']) \n","\n","    # Query OpenTargets\n","    paths = set()\n","    ot = OpenTargetsClient()\n","    for disease in tqdm(diseases):\n","        search_results = ot.search(disease)\n","        if search_results != None and len(search_results) > 0:\n","            search_result = search_results[0]\n","            if search_result['id'] == disease:\n","                if 'efo_path_codes' in search_result['data']:\n","                    for path in search_result['data']['efo_path_codes']:\n","                        paths.add('=>'.join(path))\n","                        \n","    # Turn the output we received into edges\n","    for path_str in paths:\n","        path = path_str.split('=>')\n","        for index in range(0, len(path)-1):\n","            start = path[index]\n","            end = path[index+1]\n","            graph.append([end, 'isASpecific', start])"]},{"cell_type":"markdown","metadata":{"id":"GkbhR30bCp35"},"source":["This function query Open Targets for the therapeutic area of all the diseases:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oFHDyD6aq_Ob"},"outputs":[],"source":["def add_disease_therapeutic_areas(graph):\n","    \"\"\"\n","    This function query Open Targets for the therapeutic area of all the diseases\n","    \"\"\"\n","    \n","    # Get a list of all the diseases in the graph\n","    diseases = set([t[2] for t in graph if t[1] == 'isAboutDisease']) \n","    diseases = diseases | set([t[2] for t in graph if t[1] == 'isAssociatedTo'])\n","    diseases = diseases | set([t[2] for t in graph if t[1] == 'hasGeneticClue']) \n","    diseases = diseases | set([t[2] for t in graph if t[1] == 'isASpecific']) \n","    \n","    # Query OpenTargets\n","    ot_output = {}\n","    ot = OpenTargetsClient()\n","    for disease in tqdm(diseases):\n","        ot_output[disease] = set()\n","        search_results = ot.get_disease(disease)\n","        if search_results != None and len(search_results) > 0:\n","            search_result = search_results[0]\n","            if search_result['code'].endswith(disease) and 'therapeutic_codes' in search_result:\n","                for therapeutic_code in search_result['therapeutic_codes']:\n","                        ot_output[disease].add(therapeutic_code)\n","                        \n","    # Turn the output we received into edges\n","    for (disease, areas) in ot_output.items():\n","        for area in areas:\n","            graph.append([disease, 'belongsToTherapeuticArea', area])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UFkXOBDkq_Od"},"outputs":[],"source":["def print_graph_stats(graph):\n","    resources = set([r[0] for r in graph]) | set([r[2] for r in graph])\n","    predicates = set([r[1] for r in graph])\n","    print ('Graph has {} edges, {} resources, {} predicates'.format(len(graph), len(resources), len(predicates)))\n","    display(pd.DataFrame([t for t in graph], columns=['Subject', 'Predicate', 'Object']))"]},{"cell_type":"markdown","metadata":{"id":"1KUDDUFACtSP"},"source":["We now generate the actual graph by calling the functions defined above:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JTG9p7jnq_Oj"},"outputs":[],"source":["# Step 2 => get the starting graph of paper annotations\n","graph = get_paper_annotations_graph(annotations)\n","print_graph_stats(graph)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o3l4SN9qq_Ol"},"outputs":[],"source":["# Step 3 => enrich the graph with Target - Disease links\n","connect_targets_and_diseases(graph)\n","print_graph_stats(graph)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4LPgS7L4q_On"},"outputs":[],"source":["# Step 4 => connect diseases to related diseases\n","connect_diseases_to_diseases(graph)\n","print_graph_stats(graph)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q91FAeeiq_Op"},"outputs":[],"source":["# Step 5 => add disease classification trees\n","add_disease_classification(graph)\n","print_graph_stats(graph)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0h3gV-ZZq_Or"},"outputs":[],"source":["# Step 6 => add disease therapeutic areas\n","add_disease_therapeutic_areas(graph)\n","print_graph_stats(graph)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p0aOzFVRq_Ot"},"outputs":[],"source":["# Finally, we do a last pass to remove duplicate statements\n","final_graph = [t.split('=>') for t in set(['=>'.join(t) for t in graph])]\n","print_graph_stats(final_graph)\n","\n","# and we save the graph to disk\n","graph_df = pd.DataFrame(final_graph, columns=['subject', 'predicate', 'object'])\n","graph_df.to_csv('graph.csv', index=False)"]},{"cell_type":"markdown","metadata":{"id":"lb1kWZEYq_Ow"},"source":["### A look at some of the graph content"]},{"cell_type":"markdown","metadata":{"id":"Z_oQGzkKq_Ow"},"source":["Let's see what the graph looks like for lung cancer (`MONDO_0008903`) and Covid-19 (`MONDO_0100096`) so far:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5jNGUdnRq_Ox"},"outputs":[],"source":["def get_neighbours(resource):\n","    # Extract a disease and target code=>label\n","    to_name = {}\n","    target_list = pd.read_csv('https://storage.googleapis.com/open-targets-data-releases/20.02/output/20.02_target_list.csv.gz', compression='gzip')\n","    for row in target_list.itertuples():\n","        to_name[row.ensembl_id] = row.hgnc_approved_symbol\n","    disease_list = pd.read_csv('https://storage.googleapis.com/open-targets-data-releases/20.02/output/20.02_disease_list.csv.gz', compression='gzip')\n","    for row in disease_list.itertuples():\n","        to_name[row.efo_id] = row.disease_full_name\n","    \n","    # Extract edges we may be interested in\n","    triples = [t for t in final_graph if t[0] == resource or t[2] == resource]\n","\n","    # Construct a dataframe\n","    tmp = []\n","    for t in triples:\n","        s = '{} ({})'.format(t[0], to_name.get(t[0], '?'))\n","        o = '{} ({})'.format(t[2], to_name.get(t[2], '?'))\n","        tmp.append([s,t[1],o])\n","        \n","    return pd.DataFrame(tmp, columns=['Subject', 'Predicate', 'Object'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0rNlOdQFq_Oz"},"outputs":[],"source":["display(get_neighbours('MONDO_0008903'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L6EfB2syq_O1"},"outputs":[],"source":["display(get_neighbours('MONDO_0100096'))"]},{"cell_type":"markdown","metadata":{"id":"zCrnLeBPq_O4"},"source":["# Part 3/3: Knowledge Graph Embedding Pipeline\n","\n","In this section we predict risk factors for COVID-19 using graph representation learning.\n","Each prediction is associated a confidence score. The highest the score, the more likely such prediction is true.\n","\n","Such predictions are carried out with knowledge graph embeddings, using the [AmpliGraph 1.3.1 library](https://github.com/Accenture/AmpliGraph). The knowledge grpah used is the result of step 2/3.\n","\n","**This is the last step of our contribution.**\n","\n","![covid19_pipeline_3.jpg](https://kagglecovid.s3-eu-west-1.amazonaws.com/covid19_pipeline_3.jpg)\n","\n","\n","### Knowledge Graph Embedding Training\n","\n","We train a knowledge graph embedding model using the ComplEx model [Trouillon et al. 2016] over the knowledge grpah generated at step 2/3. ComplEx reaches state-of-the-art predictive power on agreed upon [benchmark datasets](https://docs.ampligraph.org/en/latest/experiments.html), and therefore is an obvious choice for this task.\n","\n","For a comprehensive description of knowledge grpah embedding models, we suggest to read [this survey paper](https://arxiv.org/abs/2002.00388). The AmpliGraph library documention includes a [primer on the topic](https://docs.ampligraph.org/en/1.3.1/background.html).\n","\n","We train for 1,000 epochs, with an embedding size `k`=200, Adam optimizer (learning rate = 1e-4), [multiclass NLL loss](https://docs.ampligraph.org/en/1.3.1/generated/ampligraph.latent_features.NLLMulticlass.html#ampligraph.latent_features.NLLMulticlass), L3 regularizer, and early stopping criteria.\n","\n","Below we report a figure generated with [TensorFlow Embedding Projector](https://projector.tensorflow.org/). Each point in the cartesian space represents a bi-dimensional, t-SNE reduced representation of a 200-dimension embedding. Note that nodes and predicate types are embedded in the same space. The picture highlights the closest neighbours to COVID-19 in the uncompressed 200-dimensional space:\n","\n","\n","![covid19_embedding_space.jpg](https://kagglecovid.s3-eu-west-1.amazonaws.com/covid19_embedding_space.jpg)\n","\n","\n","### Sanity Check\n","To sanity-check the trained embeddings, we carve out known risk factors-disease associations for better-studied conditions (thus not including COVID-19), and we assess the predictive power of the model using agreed-upon [evaluation protocol](https://docs.ampligraph.org/en/1.3.1/generated/ampligraph.evaluation.evaluate_performance.html#ampligraph.evaluation.evaluate_performance) and [metrics](https://docs.ampligraph.org/en/1.3.1/generated/ampligraph.evaluation.mrr_score.html#ampligraph.evaluation.mrr_score). \n","\n","Such validation set includes 100 randomly selected conditions (i.e. Diseases), each connected with a number of risk factors with the `hasGeneticClue` predicate. This led to a validation set that includes 2,452 triples.\n","\n","For each of those triples, we generate 7,218 corruptions, obtained with the agreed upon [protocol]((https://docs.ampligraph.org/en/1.3.1/generated/ampligraph.evaluation.evaluate_performance.html#ampligraph.evaluation.evaluate_performance)) of replacing the object of a true triple with another concept. We use 7,218 concepts (unique diseases and risk factors) to generate 7,218 corruptions for each triple in the validation set. We then score a true triple and all its corrutpions, and we report how that true triple is ranked against its 7,218 corrutpions. We repeat the same procedure for each triple in the test set, and we compute the [mean reciprocal rank (MRR, filtered settings)]((https://docs.ampligraph.org/en/1.3.1/generated/ampligraph.evaluation.mrr_score.html#ampligraph.evaluation.mrr_score)).\n","\n","Our model reaches a validation mean reciprocal rank **MRR=0.07**.\n","\n","The table below reports our validation results. We also include [Hits@10, Hits@100](https://docs.ampligraph.org/en/1.3.1/generated/ampligraph.evaluation.hits_at_n_score.html#ampligraph.evaluation.hits_at_n_score) values, and we compare against a [baseline that returns random predictions](https://docs.ampligraph.org/en/1.3.1/generated/ampligraph.latent_features.RandomBaseline.html#randombaseline) for positive and negative triples. \n","The table shows we largely outperform the random baseline across all the metrics.\n","\n","*Note MRR, Hits@10 and Hits@100 are values between [0,1], and the higher the better.*\n","\n","|                  | MRR   | Hits@10 | Hits@100 |\n","|------------------|-------|:-------:|----------|\n","| Random Baseline  | 0.001 | 0.001   | 0.012    |\n","| Our Contribution | **0.07** | **0.137**   | **0.390**    |\n","\n","\n","As we pointed out at the top of the notebook, we had not carry out extensive hyperparametr selection yet. Such operation will likely increase the predictive power of the model, hence the quality of the embeddings used to infer risk factros for COVID-19. \n","\n","### Inference: COVID-19 Risk Factor Predictions\n","Once sanity-checked the model, we used the trained embeddings to predict the probability of existence of a number of hypothetical triples of the form `<COVID-19 hasGeneticClue ?>`, where `?` is a risk factor (that can be a characteristic, condition, or behaviour).\n","\n","These scored hypothesis are sorted from the highest scored to the lowest. A high score indicates high likelihood that the predicted risk factor affects COVID-19.\n","\n","In the cells below we describe the Knowledge Graph Embedding pipeline more in detail, and we provide the necessary source code to reproduce the experiment."]},{"cell_type":"markdown","metadata":{"id":"bxs-PdKwq_O4"},"source":["## Install the dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNYwlMBVq_O5"},"outputs":[],"source":["! conda install tensorflow-gpu'>=1.14.0,<2.0.0' -y\n","! pip install ampligraph"]},{"cell_type":"markdown","metadata":{"id":"a7GBp_Coq_O7"},"source":["## Import the necessary libs"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","id":"23rUiM6Vq_O7"},"outputs":[],"source":["import os\n","os.environ['CUDA_VISIBLE_DEVICES']='0'\n","\n","import pandas as pd\n","import numpy as np\n","np.random.seed(117)\n","from ampligraph.latent_features import ComplEx, TransE, DistMult, RandomBaseline\n","\n","from ampligraph.evaluation import evaluate_performance, mrr_score, hits_at_n_score, mr_score\n","from ampligraph.utils import save_model, restore_model\n","\n","\n","DATASET_BASE_PATH = \"/kaggle/\""]},{"cell_type":"markdown","metadata":{"id":"n754j8h7q_O9"},"source":["## Load the Knowledge graph"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","id":"itdEzJ5wq_O-","outputId":"157c4812-ca41-4984-aa29-e78220bc5b00"},"outputs":[],"source":["triples = pd.read_csv(\"graph.csv\")\n","\n","paper_diseases = set(triples[triples.predicate == 'isAboutDisease'].object)\n","paper_targets = set(triples[triples.predicate == 'isAboutTarget'].object)\n","new_triples = []\n","for row in triples.itertuples():\n","    if row.predicate == 'isAssociatedTo':\n","        if row.subject in paper_targets or row.object in paper_diseases:\n","            new_triples.append([row.subject, row.predicate, row.object])\n","    if row.predicate == 'isAboutDisease' or row.predicate == 'isAboutTarget':\n","        new_triples.append([row.subject, row.predicate, row.object])\n","    if row.predicate == 'hasGeneticClue':\n","        if row.subject in paper_diseases or row.object in paper_diseases:\n","            new_triples.append([row.subject, row.predicate, row.object])\n","    if row.predicate == 'isASpecific':\n","        new_triples.append([row.subject, row.predicate, row.object])\n","    if row.predicate == 'belongsToTherapeuticArea':\n","        new_triples.append([row.subject, row.predicate, row.object])\n","print (len(new_triples))\n","\n","\n","graph_df = pd.DataFrame(new_triples, columns=['subject', 'predicate', 'object'])\n","\n","# this line is added for making sure that the results are reproducible.\n","graph_df.sort_values(by=['subject', 'predicate', 'object'], inplace=True)\n","\n","graph_df.to_csv('COVID_KG_sample.csv', index=False)\n","\n","graph_df.head()\n","\n","print('Size of the graph:', graph_df.shape)\n","\n","print(graph_df.columns)\n","\n","print(graph_df.predicate.value_counts())"]},{"cell_type":"markdown","metadata":{"id":"NOyr9kUIq_PA"},"source":["## Construct the training set and validation set\n","\n","Our validation test consists of only the triples which have predicate `hasGeneticClue`. Hence, we can move all other triples to the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KdbG0q2_q_PA"},"outputs":[],"source":["genetic_clue_triples = graph_df[graph_df['predicate']=='hasGeneticClue']\n","train_set = graph_df[graph_df['predicate']!='hasGeneticClue'].values"]},{"cell_type":"markdown","metadata":{"id":"UBPGjA3oq_PC"},"source":["Get all the diseases that are in the current (partial) training set - so that we don't get unseen entities - i.e. at least one triple for the test set disease should exist in training set for it to get trained."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u2v0NhW-q_PD","outputId":"286bc625-f0e5-474d-bb82-59268bbc4f6f"},"outputs":[],"source":["\n","disease_list =  np.unique(np.concatenate([\n","                    np.unique(train_set[train_set[:, 1]=='isAboutDisease'][:, 2]),\n","                    np.unique(train_set[train_set[:, 1]=='isAssociatedTo'][:, 2]),\n","                ], 0))\n","\n","print('diseases in df:', len(disease_list))"]},{"cell_type":"markdown","metadata":{"id":"5_1PYquxq_PF"},"source":["From this disease list, randomly choose 100 diseases on which we will validate our model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8qEMnk0gq_PF","outputId":"ae5d0e18-9b15-4628-937e-519a2c6eb0f1"},"outputs":[],"source":["import random\n","\n","np.random.seed(117)\n","\n","test_set_diseases = np.random.choice(list(disease_list), 100).tolist()\n","\n","#test_set_diseases = set(np.random.choice(list(disease_list), 2).tolist())\n","print(test_set_diseases)"]},{"cell_type":"markdown","metadata":{"id":"P2Be2XGyq_PI"},"source":["Get the triples with predicate `hasGeneticClue` for these 100 diseases and form the test set. Move the other triples of that predicate to training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WTmYH3iwq_PI","outputId":"d5430553-3633-401f-8817-07a0487f71ed"},"outputs":[],"source":["\n","test_set = genetic_clue_triples[genetic_clue_triples[\"subject\"].isin(test_set_diseases)]\n","train_genetic_clue_triples = genetic_clue_triples[~genetic_clue_triples[\"subject\"].isin(test_set_diseases)]\n","train_set = np.concatenate([train_set, train_genetic_clue_triples], 0)\n","train_set = np.random.permutation(train_set)\n","\n","print('Train set size:', train_set.shape)\n","print('Test set size:', test_set.shape)\n","print('Full Graph size:', graph_df.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"gDcqW82oq_PK"},"source":["Now get a list of all the diseases (including risk factors) in our training set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3zJEoes7q_PL","outputId":"fad0c7d3-3052-4e1d-a659-ade832de9e5c"},"outputs":[],"source":["disease_list_full =  np.unique(np.concatenate([\n","                        np.unique(train_set[train_set[:, 1]=='isAboutDisease'][:, 2]),\n","                        np.unique(train_set[train_set[:, 1]=='isAssociatedTo'][:, 2]),\n","                        np.unique(train_set[train_set[:, 1]=='hasGeneticClue'][:, 0]),\n","                        np.unique(train_set[train_set[:, 1]=='hasGeneticClue'][:, 2]),\n","                    ], 0))\n","\n","print('diseases in df:', len(disease_list_full))"]},{"cell_type":"markdown","metadata":{"id":"Uz4gw1eqq_PN"},"source":["## Training \n","\n","Let's first train a [RandomBaseline model](https://docs.ampligraph.org/en/1.3.1/generated/ampligraph.latent_features.RandomBaseline.html#ampligraph.latent_features.RandomBaseline) and let's evaluate it on the test set. This model assigns random scores to the triples and their corruptions. So we expect to see a very bad performance (mrr) on the test set. "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["filter_triples = genetic_clue_triples.values\n","\n","random_model = RandomBaseline(seed=0)\n","\n","random_model.fit(train_set)\n","\n","ranks = evaluate_performance(test_set.values, \n","                             random_model, \n","                             filter_triples=filter_triples, \n","                             corrupt_side='o', \n","                             entities_subset=list(disease_list_full))\n","\n","print('MRR with random baseline:', mrr_score(ranks))"]},{"cell_type":"markdown","metadata":{},"source":["\n","Now, let us train a model to perform link prediction and [calibrate it](https://arxiv.org/abs/1912.10000) to return trustworthy probability estimates:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuCpTtbtq_PO"},"outputs":[],"source":["filter_triples = genetic_clue_triples.values\n","\n","\n","model = ComplEx(batches_count=15, seed=0, epochs=1000, k=200, eta=20,\n","                optimizer='adam', optimizer_params={'lr':1e-4}, \n","                verbose=True, loss='multiclass_nll',\n","                regularizer='LP', regularizer_params={'p':3, 'lambda':1e-3})\n","\n","\n","\n","early_stopping = { 'x_valid': test_set.values,\n","                   'criteria': 'mrr', \n","                  'x_filter': filter_triples, \n","                  'stop_interval': 3, \n","                  'burn_in': 50, \n","                  'corrupt_side':'o',\n","                  'corruption_entities': list(disease_list_full),\n","                  'check_interval': 50 }\n","\n","model.fit(train_set, True,early_stopping)\n","\n","ranks = evaluate_performance(test_set.values, \n","                             model, \n","                             filter_triples=filter_triples, \n","                             corrupt_side='o', \n","                             entities_subset=list(disease_list_full))\n","\n","print('MRR with trained ComplEx embedding model:', mrr_score(ranks))\n","\n","model.calibrate(train_set, positive_base_rate=0.5, epochs=100)\n","save_model(model, 'output_graph.pth')"]},{"cell_type":"markdown","metadata":{"id":"OtO3rxj7q_PQ"},"source":["The following snippet generates the [TensorBoard Embedding Projector](https://projector.tensorflow.org/) files to visualize the embedding space (see figure at the top of the notebook). \n","\n","Import the generated files into [TensorBoard Embedding Projector](https://projector.tensorflow.org/) and follow the on-screen instructions.\n","\n","[AmpliGraph documentation](https://docs.ampligraph.org/en/1.3.1/generated/ampligraph.utils.create_tensorboard_visualizations.html#ampligraph.utils.create_tensorboard_visualizations) explains how to go about in detail.\n","\n","Note this step is optional, and is required only to visually inspect the embeddings.\n","\n","\n","```python\n","from ampligraph.utils import create_tensorboard_visualizations\n","create_tensorboard_visualizations(model, 'covid19_tensorboard_files')\n","```"]},{"cell_type":"markdown","metadata":{"id":"xBG8Ztr3q_PT"},"source":["## Inference: COVID-19 Risk Factor Hypothesis Predictions \n","\n","We generate our list of hypothesis for COVID-19.\n","\n","Such hypothesis are triples in the form  `<COVID, hasGeneticClue, ?>`, where `?` is retrieved from the list of all the diseases (that include risk factors).\n","\n","We score each hypothesis using our trained model as follows:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gckRhF9-q_PT","outputId":"17a637c0-a4e4-4779-f055-74bc84e26a0a"},"outputs":[],"source":["disease_id = 'MONDO_0100096' #covid-19\n","\n","test_predicate = 'hasGeneticClue'\n","\n","hypothesis = np.concatenate([np.array([[disease_id] * disease_list_full.shape[0]]), \n","                             np.array([[test_predicate] * disease_list_full.shape[0]]),\n","                             disease_list_full[np.newaxis, :]],0).T\n","print(hypothesis.shape)\n","\n","scores = model.predict_proba(hypothesis)"]},{"cell_type":"markdown","metadata":{"id":"3dSGRzOBq_PW"},"source":["Get the file containing the disease code to name mappings for opentargets dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQCPbinXq_PW","outputId":"9cabf42f-b5cf-451f-9a88-e37af7c3110e"},"outputs":[],"source":["disease_mapping_list_df = disease_list = pd.read_csv('https://storage.googleapis.com/open-targets-data-releases/20.02/output/20.02_disease_list.csv.gz', \n","                                                     compression='gzip')\n","\n","\n","disease_mapping_list_df.head()\n"]},{"cell_type":"markdown","metadata":{"id":"R67WHulOq_PY"},"source":["## Final results\n","Merge the disease codes of the hypothesis with the above list to get the disease names and display the ranked list.\n","\n","**Such List contains the top-100 final results** (we clipped the list to reduce clutter in the notebook. **The complete list of predicted risk factors is saved to `predicted_covid19_risk_factors.csv`.**\n","\n","**Note our results are preliminary, and we aim at refining such list in phase-2, as pointed out under the future work section at the top of this notebook.**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ainzOhbaq_PZ"},"outputs":[],"source":["tested_hypothesis = pd.DataFrame(np.concatenate([hypothesis, \n","                                                 scores[:, np.newaxis]], 1), \n","                                 columns=['s','p','o','score'])\n","\n","tested_hypothesis = tested_hypothesis[tested_hypothesis['o'] != disease_id]\n","\n","tested_hypothesis = tested_hypothesis.sort_values(by='score', \n","                                                  ascending=False)\n","\n","tested_hypothesis = tested_hypothesis.merge(disease_mapping_list_df, \n","                                            how='left', \n","                                            left_on='o', \n","                                            right_on='efo_id')[['disease_full_name', 'score']]\n","\n","tested_hypothesis.columns = ['Risk Factors', 'Score']\n","\n","pd.set_option('display.max_rows', 101)\n","\n","tested_hypothesis.head(100)"]},{"cell_type":"markdown","metadata":{"id":"brimHiIRKwee"},"source":["We now save the **entire list of predictions** to disk:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVdkYUXZq_Pb"},"outputs":[],"source":["tested_hypothesis.to_csv('predicted_covid19_risk_factors.csv')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"}},"nbformat":4,"nbformat_minor":4}
